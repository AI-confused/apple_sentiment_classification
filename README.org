* Apple IOS13 Comments Sentiment Classification
** Requirements
   + pip install -r requirements.txt
   + [[https://github.com/hanxiao/bert-as-service]]
   + 需要一台能运行bert-as-service的服务器，最好有GPU，然后 pip install bert-serving-server
** Experiments
*** bert生成的词向量接入text-cnn网络
**** 先二分类为与iOS13有关&无关，再从有关的类别中进行情感分类
    1) 用dataframe读取原始数据集，清洗掉停用词和标点符号
    2) 根据有关和无关数据的分布，拆分训练集为2分类的5折，数据各自互斥,1对应0（与ios13无关）；2，3，4对应1（与iOS13有关)
    3) 每个epoch都对训练集进行shuffle
    4) 实验结果
     |---------+------------+---------+------+---------+-------+----------+----------|
     | model   | kernel_num | max_seq |   lr | dropout | batch | accuracy | macro_f1 |
     |---------+------------+---------+------+---------+-------+----------+----------|
     | roberta |        768 |     128 | 1e-3 |     0.5 |   512 |    0.781 |    0.733 |
     |---------+------------+---------+------+---------+-------+----------+----------|
    5) issues
       - textcnn使用的卷积核数目为768，卷积核大小为2
       - 由于二分类准确率太低，即便进行调参数，也很难达到误差很小的情况，并且分层分类的误差传递不可忽视，因此决定直接进行4分类
**** 充分清洗数据后进行4分类
     1) 用dataframe读取原始数据集，清洗掉停用词和标点符号
     2) 训练集去除title=''&content=''的样本，测试集title=''&content=''用‘无’代替
     3) 训练集根据原始数据分布拆分为5折训练-验证集，各自互斥
        + data_0/train.csv - data_4/train.csv
        + data_0/dev.csv - data_4/dev.csv
     4) 实验结果
      | SN | pretrained               | kernel_num | hidden | max_seq |   lr | dropout | batch | accuracy | macro_f1 |
      |----+--------------------------+------------+--------+---------+------+---------+-------+----------+----------|
      |  0 | roberta                  |        768 |    768 |      64 | 1e-3 |     0.5 |   512 |    0.569 |    0.565 |
      |  1 |                          |            |        |         |      |         |       |    0.552 |    0.545 |
      |----+--------------------------+------------+--------+---------+------+---------+-------+----------+----------|
      |  2 |                          |            |    768 |         |      |       0 |       |    0.541 |    0.544 |
      |  3 |                          |            |      0 |     128 |      |     0.5 |       |     0.55 |     0.56 |
      |----+--------------------------+------------+--------+---------+------+---------+-------+----------+----------|
      |  4 | modify loss              |            |      0 |     128 | 1e-3 |         |       |    0.589 |    0.587 |
      |----+--------------------------+------------+--------+---------+------+---------+-------+----------+----------|
      |  5 | [2, 3]kernel             |        768 |      0 |     200 |      |         |       |    0.592 |    0.591 |
      |  6 |                          |        768 |        |     200 |      |         |       |     0.57 |     0.57 |
      |  7 |                          |        768 |        |     128 |      |         |       |    0.587 |    0.587 |
      |  8 |                          |        768 |        |     128 | 1e-3 |         |       |    0.577 |    0.575 |
      |  9 |                          |        768 |        |     128 | 1e-4 |         |       |    0.567 |    0.565 |
      |  a |                          |        384 |        |     200 | 1e-3 |         |       |    0.586 |    0.586 |
      |----+--------------------------+------------+--------+---------+------+---------+-------+----------+----------|
      |  b | title&content heng xiang |        768 |        |     200 | 1e-3 |     0.5 |   512 |    0.585 |    0.587 |
      |  c | title&content zong xiang |        768 |        |         |      |         |       |    0.580 |    0.583 |
      |  d | title&content value plus |        768 |        |         |      |         |       |    0.575 |    0.574 |
      |----+--------------------------+------------+--------+---------+------+---------+-------+----------+----------|
     5) issues
        + 由于评论数据文本长度适中，因此先采用max_seq_len=64，卷积核为2*768（[[注]]:bert-as-service生成的词向量维度为768），卷积核数目为768，跑了2折数据，结果如SN-0，1
        + 适当的加大max_seq_len=128，在textcnn结构中增加一层隐藏层768个神经元，dropout=0，探索地试验了第一折数据，结果如SN-2
        + 在2的基础上，将隐藏层去掉，dropout恢复到0.5，试验第一折数据，结果如SN-3
        + 由于以上效果普遍不太好，原始数据的不同label的分布也不是完全一致，因此更改loss的权重，按照数据的比例，比例大的，权重相对小，反之权重相对大，实验结果如SN-4，的确有点提升
        + 增加卷积核的类别，由最初的2*768增加到[2,3]*768，对文本进行2和3窗口大小的卷积，在网络结构中卷积出来的矩阵先按照batch维度拼接起来，再池化处理，同时max_seq_len=200，实验了2折数据，结果如SN-5，6
        + 随后又做了一些改变参数的尝试，实验结果如SN-7，8，9，a
        + 新的想法是把title&content分别encode，再进行矩阵的拼接后，接入textcnn网络，实验结果如SN-b,c,d
**** 仅清洗非中文&非标点字符，并且只使用content内容
     1) 由于之前把title和content一起使用的效果并没有达到期待的结果，而且发现title基本是从content里摘取了前一小部分内容，因此这次只采用content；由于Bert能够有效地得到句子之间的联系，因此充分地清洗可能反而会影响效果
     2) 训练集去除title=''&content=''的样本，测试集title=''&content=''用‘无’代替
     3) 粗略清洗后按照原始数据分布分成5折训练验证集，各自互斥
        + data_0/train.csv - data_4/train.csv
        + data_0/dev.csv - data_4/dev.csv
     4) 实验结果
      | SN | description   | kernel_num | max_seq |   lr | batch | accuracy | macro_f1 |
      |----+---------------+------------+---------+------+-------+----------+----------|
      |  0 | [2,3]kernel   |        768 |     200 | 1e-3 |   512 |   0.6164 |   0.6138 |
      |  1 |               |            |         |      |       |   0.6243 |   0.6191 |
      |  2 |               |            |         |      |       |   0.6250 |   0.6204 |
      |  3 |               |            |         |      |       |   0.6243 |   0.6240 |
      |  4 |               |            |         |      |       |   0.6234 |   0.6194 |
      |----+---------------+------------+---------+------+-------+----------+----------|
      |  5 |               |            |         |      |       |   0.6188 |   0.6198 |
      |  6 | [2,3,4]kernel |            |         |      |       |   0.6233 |   0.6225 |
      |----+---------------+------------+---------+------+-------+----------+----------|
     5) issues
        + 卷积核尺寸[2,3]*768，卷积核数目768，跑了5折结果如SN：0-4
        + 卷积核尺寸设置为[2,3,4]*768，跑2折结果如SN：5-6
*** bert pytorch finetune
**** 参考了CCF的一个baseline  [[https://github.com/guoday/CCF-BDCI-Sentiment-Analysis-Baseline]]
     1) 使用bert base的预训练模型参数，按照baseline的方法，取title&content输入模型进行Bert微调，content划分为3段，每段的max_seq_len=256，实验结果没有之前textcnn高，只有0.50-0.52左右，因为结果较差，并不打算保存模型
     2) 使用roberta large的预训练参数，在上一步的基础上进行修改，去掉了title字段，只输入content进行微调，content不划分子段落，max_seq_len=256，跑完5折，准确率大致在0.60左右，实验结果记录保存在/output/roberta_large/
     3) 由于使用roberta large的实验结果并没有预期那么好，而且训练时间很长，因此继续使用bert base的预训练参数，加大训练的batch size=64，适当缩小max_seq_len=200，
** Usage-textcnn
*** 服务器启动bert-as-service
    + [[https://github.com/hanxiao/bert-as-service]] bert-as-service
    + bert-serving-start -model_dir 'your pretrained model dir' -num_worker=4 -max_seq_len=200 -pooling_strategy=NONE -port=8190
*** 清洗数据
    + cd script/
    + bash clean.sh
*** n折交叉训练
    + cd src/
    + python3 split_data.py -k=5 (5折)
*** train
    + cd script/
    + bash train.sh
*** 模型融合
    + cd src/
    + python get_test_results.py -k=5 -output=../output/model_textcnn/final.csv
