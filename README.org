* Apple IOS13 Comments Sentiment Classification
** Requirements
   + pip install -r requirements.txt
   + 需要一台能运行bert-as-service的服务器，最好有GPU，然后pip install bert-service-server
** Experiments
*** bert生成的词向量接入text-cnn网络
**** 先二分类为与iOS13有关&无关，再从有关的类别中进行情感分类
    1) 用dataframe读取原始数据集，清洗掉停用词和标点符号
    2) 拆分训练集为2分类的5折，数据各自互斥,1对应0（与ios13无关）；2，3，4对应1（与iOS13有关)
    3) 每个epoch都对训练集进行shuffle
    4) 实验结果
     |---------+------------+---------+------+---------+-------+----------+----------|
     | model   | kernel_num | max_seq |   lr | dropout | batch | accuracy | macro_f1 |
     |---------+------------+---------+------+---------+-------+----------+----------|
     | roberta |        768 |     128 | 1e-3 |     0.5 |   512 |    0.781 |    0.733 |
     |---------+------------+---------+------+---------+-------+----------+----------|
    5) issues
       - textcnn使用的卷积核数目为768，卷积核大小为2
       - 由于二分类准确率太低，即便进行调参数，也很难达到误差很小的情况，并且分层分类的误差传递不可忽视，因此决定直接进行4分类
**** 充分清洗数据后进行4分类
     1) 用dataframe读取原始数据集，清洗掉停用词和标点符号
     2) 训练集去除title=''&content=''的样本，测试集title=''&content=''用‘无’代替
     3) 训练集拆分为5折训练-验证集，各自互斥
        + data_0/train.csv - data_4/train.csv
        + data_0/dev.csv - data_4/dev.csv
     4) 实验结果
      | SN | pretrained               | kernel_num | hidden | max_seq |   lr | dropout | batch | epoch | model   238 | accuracy | macro_f1 |
      |----+--------------------------+------------+--------+---------+------+---------+-------+-------+-------------+----------+----------|
      |  0 | roberta                  |        768 |    768 |      64 | 1e-3 |     0.5 |   512 |       | model0.pkl  |    0.569 |    0.565 |
      |  1 |                          |            |        |         |      |         |       |       | model1.pkl  |    0.552 |    0.545 |
      |----+--------------------------+------------+--------+---------+------+---------+-------+-------+-------------+----------+----------|
      |  2 |                          |            |    768 |         |      |       0 |       |       | model11.pkl |    0.541 |    0.544 |
      |  3 |                          |            |      0 |     128 |      |     0.5 |       |       | model12.pkl |     0.55 |     0.56 |
      |----+--------------------------+------------+--------+---------+------+---------+-------+-------+-------------+----------+----------|
      |  4 | modify loss              |            |      0 |     128 | 1e-3 |         |       |    20 | model20.pkl |    0.589 |    0.587 |
      |----+--------------------------+------------+--------+---------+------+---------+-------+-------+-------------+----------+----------|
      |  5 | [2, 3]kernel             |    tag:cat |      0 |     200 |      |         |       |       | model30.pkl |    0.592 |    0.591 |
      |  6 |                          |            |        |     200 |      |         |       |       | model31.pkl |     0.57 |     0.57 |
      |  7 |                          |            |        |     128 |      |         |       |       | model40.pkl |    0.587 |    0.587 |
      |  8 |                          |            |        |     128 | 1e-3 |         |       |       | model41.pkl |    0.577 |    0.575 |
      |  9 |                          |            |        |     128 | 1e-4 |         |       |       | model50.pkl |    0.567 |    0.565 |
      |  a |                          |        384 |        |     200 | 1e-3 |         |       |       | model60.pkl |    0.586 |    0.586 |
      |----+--------------------------+------------+--------+---------+------+---------+-------+-------+-------------+----------+----------|
      |  b | title&content heng xiang |            |        |         |      |         |       |       |             |          |          |
      |  c | title&content zong xiang |            |        |         |      |         |       |       |             |          |          |
      |  d | title&content value plus |        768 |        |         |      |         |       |       | model70.pkl |    0.575 |    0.574 |
      |----+--------------------------+------------+--------+---------+------+---------+-------+-------+-------------+----------+----------|
      |  e | modify loss              |            |        |         | 1e-2 |         |       |       | /           |    0.537 |    0.539 |
     5) issues
        + 由于评论数据文本长度适中，因此先采用max_seq_len=64，卷积核为2*768（[[注]]:bert-as-service生成的词向量维度为768），卷积核数目为768，跑了2折数据，结果如SN-0，1
        + 适当的加大max_seq_len=128，在textcnn结构中增加一层隐藏层768个神经元，dropout=0，探索地试验了第一折数据，结果如SN-2
        + 在2的基础上，将隐藏层去掉，dropout恢复到0.5，试验第一折数据，结果如SN-3
        + 由于以上效果普遍不太好，原始数据的不同label的分布也不是完全一致，因此更改loss的权重，按照数据的比例，比例大的，权重相对小，反之权重相对大，实验结果如SN-4，的确有点提升
        + 增加卷积核的类别，由最初的2*768增加到[2,3]*768，对文本进行2和3窗口大小的卷积，在网络结构中卷积出来的矩阵先按照batch维度拼接起来，再池化处理，同时max_seq_len=200，实验了2折数据，结果如SN-5，6
        + 

*** bert pytorch finetune  
     
