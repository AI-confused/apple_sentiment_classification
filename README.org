* Apple IOS13 Comments Sentiment Classification
** Requirements
   + pip install -r requirements.txt
   + 需要一台能运行bert-as-service的服务器，最好有GPU，然后pip install bert-service-server
** Experiments
*** bert生成的词向量接入text-cnn网络
**** 先二分类为与iOS13有关&无关，再从有关的类别中进行情感分类
    + 拆分训练集为2分类的5折，数据各自互斥,1对应0（与ios13无关）；2，3，4对应1（与iOS13有关)
    + 每个epoch都对训练集进行shuffle
    + text-cnn
     |---------+------------+---------+------+---------+-------+----------+----------|
     | model   | kernel_num | max_seq |   lr | dropout | batch | accuracy | macro_f1 |
     |---------+------------+---------+------+---------+-------+----------+----------|
     | roberta |        768 |     128 | 1e-3 |     0.5 |   512 |    0.781 |    0.733 |
     |---------+------------+---------+------+---------+-------+----------+----------|
    + issues
     - textcnn使用的卷积核数目为768，卷积核大小为2
     - 由于二分类准确率太低，即便进行调参数，也很难达到误差很小的情况，并且分层分类的误差传递不可忽视，因此决定直接进行4分类
**** 直接4分类
     1) 用dataframe读取原始数据集，清洗掉停用词和标点符号
     2) 训练集去除title=''&content=''的样本，测试集title=''&content=''用‘无’代替
     3) 训练集拆分为5折训练-验证集，各自互斥
        + data_0/train.csv-data_4/train.csv
        + data_0/dev.csv-data_4/dev.csv
     4) 实验结果
      | description              | kernel_num | hidden | max_seq |   lr | dropout | batch | accuracy | macro_f1 |
      |--------------------------+------------+--------+---------+------+---------+-------+----------+----------|
      | roberta                  |        768 |    768 |      64 | 1e-3 |     0.5 |   512 |    0.569 |    0.565 |
      |                          |            |        |         |      |         |       |    0.552 |    0.545 |
      |--------------------------+------------+--------+---------+------+---------+-------+----------+----------|
      |                          |            |    768 |         |      |       0 |       |    0.541 |    0.544 |
      |                          |            |      0 |     128 |      |     0.5 |       |     0.55 |     0.56 |
      |--------------------------+------------+--------+---------+------+---------+-------+----------+----------|
      | modify loss              |            |      0 |     128 | 1e-3 |         |       |    0.589 |    0.587 |
      |--------------------------+------------+--------+---------+------+---------+-------+----------+----------|
      | [2, 3]kernel             |    tag:cat |      0 |     200 |      |         |       |    0.592 |    0.591 |
      |                          |            |        |     200 |      |         |       |     0.57 |     0.57 |
      |                          |            |        |     128 |      |         |       |    0.587 |    0.587 |
      |                          |            |        |     128 | 1e-3 |         |       |    0.577 |    0.575 |
      |                          |            |        |     128 | 1e-4 |         |       |    0.567 |    0.565 |
      |                          |        384 |        |     200 | 1e-3 |         |       |    0.586 |    0.586 |
      |--------------------------+------------+--------+---------+------+---------+-------+----------+----------|
      | title&content heng xiang |            |        |         |      |         |       |          |          |
      | title&content zong xiang |            |        |         |      |         |       |          |          |
      | title&content value plus |        768 |        |         |      |         |       |    0.575 |    0.574 |
      |--------------------------+------------+--------+---------+------+---------+-------+----------+----------|
      | modify loss              |            |        |         | 1e-2 |         |       |    0.537 |    0.539 |
*** bert pytorch finetune  
     
