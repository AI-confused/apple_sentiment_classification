* Apple IOS13 Comments Sentiment Classification
** Requirements
   + pip install -r requirements.txt
   + 需要一台能运行bert-as-service的服务器，最好有GPU，然后pip install bert-service-server
** Experiments
*** bert生成的词向量接入text-cnn网络
**** 先二分类为与iOS13有关&无关，再从有关的类别中进行情感分类
    1) 用dataframe读取原始数据集，清洗掉停用词和标点符号
    2) 根据有关和无关数据的分布，拆分训练集为2分类的5折，数据各自互斥,1对应0（与ios13无关）；2，3，4对应1（与iOS13有关)
    3) 每个epoch都对训练集进行shuffle
    4) 实验结果
     |---------+------------+---------+------+---------+-------+----------+----------|
     | model   | kernel_num | max_seq |   lr | dropout | batch | accuracy | macro_f1 |
     |---------+------------+---------+------+---------+-------+----------+----------|
     | roberta |        768 |     128 | 1e-3 |     0.5 |   512 |    0.781 |    0.733 |
     |---------+------------+---------+------+---------+-------+----------+----------|
    5) issues
       - textcnn使用的卷积核数目为768，卷积核大小为2
       - 由于二分类准确率太低，即便进行调参数，也很难达到误差很小的情况，并且分层分类的误差传递不可忽视，因此决定直接进行4分类
**** 充分清洗数据后进行4分类
     1) 用dataframe读取原始数据集，清洗掉停用词和标点符号
     2) 训练集去除title=''&content=''的样本，测试集title=''&content=''用‘无’代替
     3) 训练集根据原始数据分布拆分为5折训练-验证集，各自互斥
        + data_0/train.csv - data_4/train.csv
        + data_0/dev.csv - data_4/dev.csv
     4) 实验结果
      | SN | pretrained               | kernel_num | hidden | max_seq |   lr | dropout | batch | accuracy | macro_f1 |
      |----+--------------------------+------------+--------+---------+------+---------+-------+----------+----------|
      |  0 | roberta                  |        768 |    768 |      64 | 1e-3 |     0.5 |   512 |    0.569 |    0.565 |
      |  1 |                          |            |        |         |      |         |       |    0.552 |    0.545 |
      |----+--------------------------+------------+--------+---------+------+---------+-------+----------+----------|
      |  2 |                          |            |    768 |         |      |       0 |       |    0.541 |    0.544 |
      |  3 |                          |            |      0 |     128 |      |     0.5 |       |     0.55 |     0.56 |
      |----+--------------------------+------------+--------+---------+------+---------+-------+----------+----------|
      |  4 | modify loss              |            |      0 |     128 | 1e-3 |         |       |    0.589 |    0.587 |
      |----+--------------------------+------------+--------+---------+------+---------+-------+----------+----------|
      |  5 | [2, 3]kernel             |        768 |      0 |     200 |      |         |       |    0.592 |    0.591 |
      |  6 |                          |        768 |        |     200 |      |         |       |     0.57 |     0.57 |
      |  7 |                          |        768 |        |     128 |      |         |       |    0.587 |    0.587 |
      |  8 |                          |        768 |        |     128 | 1e-3 |         |       |    0.577 |    0.575 |
      |  9 |                          |        768 |        |     128 | 1e-4 |         |       |    0.567 |    0.565 |
      |  a |                          |        384 |        |     200 | 1e-3 |         |       |    0.586 |    0.586 |
      |----+--------------------------+------------+--------+---------+------+---------+-------+----------+----------|
      |  b | title&content heng xiang |        768 |        |     200 | 1e-3 |     0.5 |   512 |    0.585 |    0.587 |
      |  c | title&content zong xiang |        768 |        |         |      |         |       |    0.580 |    0.583 |
      |  d | title&content value plus |        768 |        |         |      |         |       |    0.575 |    0.574 |
      |----+--------------------------+------------+--------+---------+------+---------+-------+----------+----------|
     5) issues
        + 由于评论数据文本长度适中，因此先采用max_seq_len=64，卷积核为2*768（[[注]]:bert-as-service生成的词向量维度为768），卷积核数目为768，跑了2折数据，结果如SN-0，1
        + 适当的加大max_seq_len=128，在textcnn结构中增加一层隐藏层768个神经元，dropout=0，探索地试验了第一折数据，结果如SN-2
        + 在2的基础上，将隐藏层去掉，dropout恢复到0.5，试验第一折数据，结果如SN-3
        + 由于以上效果普遍不太好，原始数据的不同label的分布也不是完全一致，因此更改loss的权重，按照数据的比例，比例大的，权重相对小，反之权重相对大，实验结果如SN-4，的确有点提升
        + 增加卷积核的类别，由最初的2*768增加到[2,3]*768，对文本进行2和3窗口大小的卷积，在网络结构中卷积出来的矩阵先按照batch维度拼接起来，再池化处理，同时max_seq_len=200，实验了2折数据，结果如SN-5，6
        + 随后又做了一些改变参数的尝试，实验结果如SN-7，8，9，a
        + 新的想法是把title&content分别encode，再进行矩阵的拼接后，接入textcnn网络，实验结果如SN-b,c,d
**** 仅清洗非中文&非标点字符，并且只使用content内容
     1) 由于之前把title和content一起使用的效果并没有达到期待的结果，而且发现title基本是从content里摘取了前一小部分内容，因此这次只采用content；由于Bert能够有效地得到句子之间的联系，因此充分地清洗可能反而会影响效果
     2) 训练集去除title=''&content=''的样本，测试集title=''&content=''用‘无’代替
     3) 粗略清洗后按照原始数据分布分成5折训练验证集，各自互斥
        + data_0/train.csv - data_4/train.csv
        + data_0/dev.csv - data_4/dev.csv
     4) 实验结果
       | SN | description   | kernel_num | max_seq |   lr | dropout | batch | epoch | model   238  | accuracy | macro_f1 |
       |----+---------------+------------+---------+------+---------+-------+-------+--------------+----------+----------|
       |  0 | [2,3]kernel   |        768 |     200 | 1e-3 |     0.5 |   512 |    10 | modeln0.pkl  |   0.6164 |   0.6138 |
       |  1 |               |            |         |      |         |       |       | modeln1.pkl  |   0.6243 |   0.6191 |
       |  2 |               |            |         |      |         |       |       | modeln2.pkl  |   0.6250 |   0.6204 |
       |  3 |               |            |         |      |         |       |       | modeln3.pkl  |   0.6243 |   0.6240 |
       |  4 |               |            |         |      |         |       |       | modeln4.pkl  |   0.6234 |   0.6194 |
       |----+---------------+------------+---------+------+---------+-------+-------+--------------+----------+----------|
       |  5 | [2,3]  cat:2  |            |         |      |         |       |       | modelz0.pkl  |   0.6109 |   0.6170 |
       |----+---------------+------------+---------+------+---------+-------+-------+--------------+----------+----------|
       |  6 |               |            |         |      |         |       |       | modeln_0.pkl |   0.6188 |   0.6198 |
       |  7 | [2,3,4]kernel |            |         |      |         |       |       | modeln_1.pkl |   0.6233 |   0.6225 |
       |----+---------------+------------+---------+------+---------+-------+-------+--------------+----------+----------|

*** bert pytorch finetune  
     
